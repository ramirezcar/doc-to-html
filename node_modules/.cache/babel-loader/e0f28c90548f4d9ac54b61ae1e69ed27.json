{"ast":null,"code":"var Token = require(\"./Token\");\n\nvar StringSource = require(\"./StringSource\");\n\nexports.RegexTokeniser = RegexTokeniser;\n\nfunction RegexTokeniser(rules) {\n  rules = rules.map(function (rule) {\n    return {\n      name: rule.name,\n      regex: new RegExp(rule.regex.source, \"g\")\n    };\n  });\n\n  function tokenise(input, description) {\n    var source = new StringSource(input, description);\n    var index = 0;\n    var tokens = [];\n\n    while (index < input.length) {\n      var result = readNextToken(input, index, source);\n      index = result.endIndex;\n      tokens.push(result.token);\n    }\n\n    tokens.push(endToken(input, source));\n    return tokens;\n  }\n\n  function readNextToken(string, startIndex, source) {\n    for (var i = 0; i < rules.length; i++) {\n      var regex = rules[i].regex;\n      regex.lastIndex = startIndex;\n      var result = regex.exec(string);\n\n      if (result) {\n        var endIndex = startIndex + result[0].length;\n\n        if (result.index === startIndex && endIndex > startIndex) {\n          var value = result[1];\n          var token = new Token(rules[i].name, value, source.range(startIndex, endIndex));\n          return {\n            token: token,\n            endIndex: endIndex\n          };\n        }\n      }\n    }\n\n    var endIndex = startIndex + 1;\n    var token = new Token(\"unrecognisedCharacter\", string.substring(startIndex, endIndex), source.range(startIndex, endIndex));\n    return {\n      token: token,\n      endIndex: endIndex\n    };\n  }\n\n  function endToken(input, source) {\n    return new Token(\"end\", null, source.range(input.length, input.length));\n  }\n\n  return {\n    tokenise: tokenise\n  };\n}","map":{"version":3,"sources":["C:/Users/Carlos/Desktop/Coding/doc-to-html/node_modules/lop/lib/regex-tokeniser.js"],"names":["Token","require","StringSource","exports","RegexTokeniser","rules","map","rule","name","regex","RegExp","source","tokenise","input","description","index","tokens","length","result","readNextToken","endIndex","push","token","endToken","string","startIndex","i","lastIndex","exec","value","range","substring"],"mappings":"AAAA,IAAIA,KAAK,GAAGC,OAAO,CAAC,SAAD,CAAnB;;AACA,IAAIC,YAAY,GAAGD,OAAO,CAAC,gBAAD,CAA1B;;AAEAE,OAAO,CAACC,cAAR,GAAyBA,cAAzB;;AAEA,SAASA,cAAT,CAAwBC,KAAxB,EAA+B;AAC3BA,EAAAA,KAAK,GAAGA,KAAK,CAACC,GAAN,CAAU,UAASC,IAAT,EAAe;AAC7B,WAAO;AACHC,MAAAA,IAAI,EAAED,IAAI,CAACC,IADR;AAEHC,MAAAA,KAAK,EAAE,IAAIC,MAAJ,CAAWH,IAAI,CAACE,KAAL,CAAWE,MAAtB,EAA8B,GAA9B;AAFJ,KAAP;AAIH,GALO,CAAR;;AAOA,WAASC,QAAT,CAAkBC,KAAlB,EAAyBC,WAAzB,EAAsC;AAClC,QAAIH,MAAM,GAAG,IAAIT,YAAJ,CAAiBW,KAAjB,EAAwBC,WAAxB,CAAb;AACA,QAAIC,KAAK,GAAG,CAAZ;AACA,QAAIC,MAAM,GAAG,EAAb;;AAEA,WAAOD,KAAK,GAAGF,KAAK,CAACI,MAArB,EAA6B;AACzB,UAAIC,MAAM,GAAGC,aAAa,CAACN,KAAD,EAAQE,KAAR,EAAeJ,MAAf,CAA1B;AACAI,MAAAA,KAAK,GAAGG,MAAM,CAACE,QAAf;AACAJ,MAAAA,MAAM,CAACK,IAAP,CAAYH,MAAM,CAACI,KAAnB;AACH;;AAEDN,IAAAA,MAAM,CAACK,IAAP,CAAYE,QAAQ,CAACV,KAAD,EAAQF,MAAR,CAApB;AACA,WAAOK,MAAP;AACH;;AAED,WAASG,aAAT,CAAuBK,MAAvB,EAA+BC,UAA/B,EAA2Cd,MAA3C,EAAmD;AAC/C,SAAK,IAAIe,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGrB,KAAK,CAACY,MAA1B,EAAkCS,CAAC,EAAnC,EAAuC;AACnC,UAAIjB,KAAK,GAAGJ,KAAK,CAACqB,CAAD,CAAL,CAASjB,KAArB;AACAA,MAAAA,KAAK,CAACkB,SAAN,GAAkBF,UAAlB;AACA,UAAIP,MAAM,GAAGT,KAAK,CAACmB,IAAN,CAAWJ,MAAX,CAAb;;AAEA,UAAIN,MAAJ,EAAY;AACR,YAAIE,QAAQ,GAAGK,UAAU,GAAGP,MAAM,CAAC,CAAD,CAAN,CAAUD,MAAtC;;AACA,YAAIC,MAAM,CAACH,KAAP,KAAiBU,UAAjB,IAA+BL,QAAQ,GAAGK,UAA9C,EAA0D;AACtD,cAAII,KAAK,GAAGX,MAAM,CAAC,CAAD,CAAlB;AACA,cAAII,KAAK,GAAG,IAAItB,KAAJ,CACRK,KAAK,CAACqB,CAAD,CAAL,CAASlB,IADD,EAERqB,KAFQ,EAGRlB,MAAM,CAACmB,KAAP,CAAaL,UAAb,EAAyBL,QAAzB,CAHQ,CAAZ;AAKA,iBAAO;AAACE,YAAAA,KAAK,EAAEA,KAAR;AAAeF,YAAAA,QAAQ,EAAEA;AAAzB,WAAP;AACH;AACJ;AACJ;;AACD,QAAIA,QAAQ,GAAGK,UAAU,GAAG,CAA5B;AACA,QAAIH,KAAK,GAAG,IAAItB,KAAJ,CACR,uBADQ,EAERwB,MAAM,CAACO,SAAP,CAAiBN,UAAjB,EAA6BL,QAA7B,CAFQ,EAGRT,MAAM,CAACmB,KAAP,CAAaL,UAAb,EAAyBL,QAAzB,CAHQ,CAAZ;AAKA,WAAO;AAACE,MAAAA,KAAK,EAAEA,KAAR;AAAeF,MAAAA,QAAQ,EAAEA;AAAzB,KAAP;AACH;;AAED,WAASG,QAAT,CAAkBV,KAAlB,EAAyBF,MAAzB,EAAiC;AAC7B,WAAO,IAAIX,KAAJ,CACH,KADG,EAEH,IAFG,EAGHW,MAAM,CAACmB,KAAP,CAAajB,KAAK,CAACI,MAAnB,EAA2BJ,KAAK,CAACI,MAAjC,CAHG,CAAP;AAKH;;AAED,SAAO;AACHL,IAAAA,QAAQ,EAAEA;AADP,GAAP;AAGH","sourcesContent":["var Token = require(\"./Token\");\nvar StringSource = require(\"./StringSource\");\n\nexports.RegexTokeniser = RegexTokeniser;\n\nfunction RegexTokeniser(rules) {\n    rules = rules.map(function(rule) {\n        return {\n            name: rule.name,\n            regex: new RegExp(rule.regex.source, \"g\")\n        };\n    });\n    \n    function tokenise(input, description) {\n        var source = new StringSource(input, description);\n        var index = 0;\n        var tokens = [];\n    \n        while (index < input.length) {\n            var result = readNextToken(input, index, source);\n            index = result.endIndex;\n            tokens.push(result.token);\n        }\n        \n        tokens.push(endToken(input, source));\n        return tokens;\n    }\n\n    function readNextToken(string, startIndex, source) {\n        for (var i = 0; i < rules.length; i++) {\n            var regex = rules[i].regex;\n            regex.lastIndex = startIndex;\n            var result = regex.exec(string);\n            \n            if (result) {\n                var endIndex = startIndex + result[0].length;\n                if (result.index === startIndex && endIndex > startIndex) {\n                    var value = result[1];\n                    var token = new Token(\n                        rules[i].name,\n                        value,\n                        source.range(startIndex, endIndex)\n                    );\n                    return {token: token, endIndex: endIndex};\n                }\n            }\n        }\n        var endIndex = startIndex + 1;\n        var token = new Token(\n            \"unrecognisedCharacter\",\n            string.substring(startIndex, endIndex),\n            source.range(startIndex, endIndex)\n        );\n        return {token: token, endIndex: endIndex};\n    }\n    \n    function endToken(input, source) {\n        return new Token(\n            \"end\",\n            null,\n            source.range(input.length, input.length)\n        );\n    }\n    \n    return {\n        tokenise: tokenise\n    }\n}\n\n\n"]},"metadata":{},"sourceType":"script"}